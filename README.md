# ğŸ‹ï¸ Synrgy - Plateforme de Coaching Sportif avec IA Ollama

## ğŸ¯ Description

Synrgy est une plateforme complÃ¨te de coaching sportif qui utilise l'IA Ollama local pour gÃ©nÃ©rer des plans d'entraÃ®nement et nutritionnels personnalisÃ©s.

## âœ¨ FonctionnalitÃ©s

- ğŸ¤– **IA Ollama intÃ©grÃ©e** : GÃ©nÃ©ration de plans avec llama3.2:1b
- ğŸƒ **Plans d'entraÃ®nement** : Programmes personnalisÃ©s selon objectifs
- ğŸ¥— **Plans nutritionnels** : RÃ©gimes adaptÃ©s aux besoins
- ğŸ‘¥ **Multi-rÃ´les** : Coach et athlÃ¨te avec dashboards dÃ©diÃ©s
- ğŸ“Š **Analytics** : Suivi des performances et progression
- ğŸ¨ **Interface moderne** : Design responsive avec Tailwind CSS

## ğŸš€ Installation et DÃ©marrage

### PrÃ©requis
- Node.js 18+
- Ollama installÃ©
- ModÃ¨le llama3.2:1b tÃ©lÃ©chargÃ©

### Installation
```bash
# Cloner le projet
git clone https://github.com/ethan-plnqrt/synrgy-coachpro.git
cd synrgy-coachpro

# Installer les dÃ©pendances
npm install

# DÃ©marrer Ollama
ollama serve

# Installer le modÃ¨le IA
ollama pull llama3.2:1b

# DÃ©marrer le serveur
npm run dev:server
```

### DÃ©marrage automatique
```bash
# Script de dÃ©marrage complet
./start-synrgy-ollama.sh
```

## ğŸ”§ Configuration

### Variables d'environnement (.env)
```env
AI_PROVIDER=ollama
OLLAMA_API_URL=http://localhost:11434
MODEL_NAME=llama3.2:1b
DATABASE_URL=file:./dev.db
SESSION_SECRET=your-secret-key
TEST_MODE=false
```

## ğŸ“¡ API Endpoints

### IA et Coaching
- `POST /api/ask` - Chat gÃ©nÃ©ral avec l'IA
- `POST /api/nutrition/generate` - GÃ©nÃ©ration de plans nutritionnels
- `POST /api/trainingPlan/generate` - GÃ©nÃ©ration de programmes d'entraÃ®nement

### Authentification
- `POST /api/auth/register` - Inscription
- `POST /api/auth/login` - Connexion
- `GET /api/auth/me` - Profil utilisateur

### Gestion des programmes
- `GET /api/programs` - Liste des programmes
- `POST /api/programs` - CrÃ©er un programme
- `DELETE /api/programs/:id` - Supprimer un programme

## ğŸ§ª Tests

### Test de l'intÃ©gration Ollama
```bash
npx tsx test-ollama-complete.js
```

### Test des endpoints
```bash
# Chat IA
curl -X POST http://localhost:5000/api/ask \
  -H "Content-Type: application/json" \
  -d '{"content":"Bonjour, aide-moi avec mon entraÃ®nement"}'

# Plan nutrition
curl -X POST http://localhost:5000/api/nutrition/generate \
  -H "Content-Type: application/json" \
  -d '{"goal":"perte de poids","level":"dÃ©butant","weight":70,"height":175,"activity":"modÃ©rÃ©e","preferences":"vÃ©gÃ©tarien"}'
```

## ğŸ—ï¸ Architecture

### Backend
- **Express.js** : Serveur API
- **TypeScript** : Langage principal
- **SQLite** : Base de donnÃ©es locale
- **Ollama** : IA locale

### Frontend
- **React 18** : Interface utilisateur
- **Tailwind CSS** : Styling
- **Radix UI** : Composants
- **React Query** : Gestion d'Ã©tat

## ğŸ“ Structure du Projet

```
synrgy-coachpro/
â”œâ”€â”€ client/                 # Frontend React
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/     # Composants UI
â”‚   â”‚   â”œâ”€â”€ pages/          # Pages de l'application
â”‚   â”‚   â”œâ”€â”€ hooks/          # Hooks personnalisÃ©s
â”‚   â”‚   â””â”€â”€ lib/            # Utilitaires
â”œâ”€â”€ server/                 # Backend Express
â”‚   â”œâ”€â”€ ai/                 # IntÃ©gration Ollama
â”‚   â”œâ”€â”€ routes/             # Routes API
â”‚   â””â”€â”€ storage.ts          # Gestion base de donnÃ©es
â”œâ”€â”€ shared/                 # SchÃ©mas partagÃ©s
â””â”€â”€ docs/                   # Documentation
```

## ğŸ”„ Migration depuis OpenAI

Le projet a Ã©tÃ© migrÃ© d'OpenAI vers Ollama local :

- âœ… DÃ©pendance OpenAI supprimÃ©e
- âœ… Module Ollama crÃ©Ã© (`/server/ai/ollama.ts`)
- âœ… Routes adaptÃ©es pour Ollama
- âœ… Configuration mise Ã  jour
- âœ… Tests validÃ©s

## ğŸš¨ DÃ©pannage

### Ollama non disponible
```bash
# VÃ©rifier le service
ollama serve

# VÃ©rifier les modÃ¨les
ollama list

# Installer le modÃ¨le
ollama pull llama3.2:1b
```

### Port occupÃ©
```bash
# Trouver le processus
lsof -i :5000

# ArrÃªter le processus
kill -9 <PID>
```

## ğŸ“Š Performance

- **ModÃ¨le IA** : llama3.2:1b (~1-2GB RAM)
- **Temps de rÃ©ponse** : 2-8 secondes
- **Base de donnÃ©es** : SQLite local
- **Ports** : 5000 (serveur), 11434 (Ollama)

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche feature
3. Commit les changements
4. Push vers la branche
5. Ouvrir une Pull Request

## ğŸ“„ Licence

MIT License - Voir le fichier LICENSE pour plus de dÃ©tails.

## ğŸ“ Support

Pour toute question ou problÃ¨me :
- Ouvrir une issue sur GitHub
- Consulter la documentation dans `/docs`
- VÃ©rifier les logs du serveur

---

**DÃ©veloppÃ© avec â¤ï¸ pour la communautÃ© sportive**