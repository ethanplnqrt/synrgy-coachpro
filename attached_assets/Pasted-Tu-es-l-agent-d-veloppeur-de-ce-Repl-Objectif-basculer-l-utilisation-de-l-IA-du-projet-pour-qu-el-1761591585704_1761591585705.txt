Tu es l'agent développeur de ce Repl. Objectif : basculer l'utilisation de l'IA du projet pour qu'elle passe **uniquement** par les Replit AI Integrations (utilisant mes crédits Replit), et non par ma clé OpenAI personnelle. Fais les changements suivants de façon sécurisée, peu coûteuse, et testable. Ne dépasse pas inutilement les crédits : optimise les appels et limite les tokens.

1) Supprime la variable d'environnement OPENAI_API_KEY si elle existe dans les secrets du projet. Ne laisse aucune clé personnelle dans le code ou dans un fichier visible.

2) Ajoute une variable d'environnement (ou flag de config) nommée USE_REPLIT_AI=true et assure-toi que le code lit ce flag pour choisir l'implémentation IA.

3) Remplace toutes les occurrences des appels directs à l'API OpenAI (fetch vers api.openai.com, openai.ChatCompletion/create, usage de OPENAI_API_KEY, etc.) par des appels à l’intégration Replit AI (utilise l’API interne Replit AI disponible dans cet environnement — ex: utiliser l'interface `ai.chat()` ou la méthode recommandée par Replit). 
   - Si le projet contient un utilitaire `openaiClient` ou similaire, remplace-le par un wrapper `aiClient` qui utilise Replit AI lorsque USE_REPLIT_AI=true, sinon conserve une implémentation fallback qui signale l'absence de clé.
   - Garde le comportement et les signatures compatibles pour le frontend (même format de réponse).

4) Implémente ces règles d'économie de crédit :
   - Ajoute un prompt système concis et efficace pour le coach (2–3 phrases maximales). Evite de renvoyer l'historique entier à chaque requête : n'envoyer que le contexte essentiel (dernier message + résumé court de la conversation si nécessaire).
   - Fixe `max_tokens` à une valeur raisonnable (ex: 512) et expose une variable d'environnement MAX_TOKENS pour pouvoir ajuster.
   - Ajoute une logique de throttling/rate-limit (ex: 30 requêtes utilisateur / minute max) et un cache simple pour réponses répétées (cache en mémoire pendant N minutes).
   - Logue la consommation approximative (nombre de tokens estimés par requête) dans un fichier/console `ai_usage.log` pour que je puisse monitorer la dépense.

5) Dans le backend, crée ou mets à jour l'endpoint `/api/ai/chat` pour :
   - détecter USE_REPLIT_AI,
   - appeler la logique Replit AI wrapper,
   - retourner la même structure JSON attendue par le frontend (messages[], etc.),
   - gérer proprement les erreurs (500 → message lisible client : "Service IA temporairement indisponible, réessaye dans quelques instants").

6) Dans le frontend, vérifie que le composant ChatIA consomme `/api/ai/chat` et :
   - affiche un loader pendant la génération,
   - gère les erreurs lisiblement,
   - limite l'envoi multiple (désactive le bouton pendant la requête),
   - n'envoie pas l'intégralité de l'historique à chaque fois (envoyer uniquement id de conversation + dernier message ; si résumé requis, appelle un endpoint de résumé).

7) Ajoute un test automatique qui simule une requête cliente (ex: "Je veux un programme pour perdre 5 kg") :
   - Exécute le test end-to-end côté serveur (curl local) pour vérifier réponse,
   - Ecris les logs de test (status, latence, tokens estimés) dans la console.

8) Après avoir effectué ces modifications :
   - Redémarre proprement l'application,
   - Lance le test end-to-end,
   - Rends-moi un rapport synthétique dans le chat indiquant : fichiers modifiés, endpoints touchés, résultats du test (status + extrait de la réponse IA), estimation de coût par requête (tokens max estimés).

Contraintes importantes :
- Ne stocke **jamais** la clé OpenAI dans un fichier visible (README, .env commité, etc.).
- N'appelle pas OpenAI directement tant que je n'ai pas explicitement demandé à repasser à ma clé perso.
- Optimise pour coûts faibles et robustesse.

Merci — exécute et renvoie le rapport automatiquement une fois terminé.
