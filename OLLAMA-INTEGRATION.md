# Synrgy - Int√©gration Ollama

## üéØ Configuration Ollama

Synrgy a √©t√© adapt√© pour fonctionner avec Ollama local au lieu d'OpenAI.

### Variables d'environnement (.env)

```env
# Configuration Ollama
AI_PROVIDER=ollama
OLLAMA_API_URL=http://localhost:11434
MODEL_NAME=llama3.2:1b

# Configuration serveur
SESSION_SECRET=your-secret-key-here
TEST_MODE=false
DATABASE_URL=file:./dev.db

# Configuration Stripe (optionnel)
STRIPE_SECRET_KEY=your-stripe-secret-key
```

### Installation et d√©marrage

1. **Installer Ollama** (si pas d√©j√† fait) :
   ```bash
   # macOS
   brew install ollama
   
   # Ou t√©l√©charger depuis https://ollama.ai
   ```

2. **D√©marrer le service Ollama** :
   ```bash
   ollama serve
   ```

3. **Installer le mod√®le** :
   ```bash
   ollama pull llama3.2:1b
   ```

4. **Installer les d√©pendances** :
   ```bash
   npm install
   ```

5. **D√©marrer le serveur Synrgy** :
   ```bash
   npm run dev:server
   ```

## üîß Architecture

### Fichiers modifi√©s

- **`/server/ai/ollama.ts`** : Nouveau module d'int√©gration Ollama
- **`/server/openai.ts`** : Modifi√© pour utiliser Ollama au lieu d'OpenAI
- **`/server/routes.ts`** : Endpoints IA adapt√©s pour Ollama
- **`/server/routes/nutrition.ts`** : Route nutrition avec Ollama
- **`/server/routes/trainingPlan.ts`** : Route training plan avec Ollama
- **`package.json`** : D√©pendance OpenAI supprim√©e, node-fetch ajout√©e

### Endpoints IA disponibles

- **`POST /api/ask`** : Chat g√©n√©ral avec l'IA
- **`POST /api/nutrition/generate`** : G√©n√©ration de plans nutritionnels
- **`POST /api/trainingPlan/generate`** : G√©n√©ration de programmes d'entra√Ænement

## üß™ Tests

### Test automatique

```bash
node test-ollama-complete.js
```

### Test manuel des endpoints

```bash
# Test chat g√©n√©ral
curl -X POST http://localhost:5000/api/ask \
  -H "Content-Type: application/json" \
  -d '{"content":"Bonjour, peux-tu m'aider avec mon entra√Ænement ?"}'

# Test nutrition
curl -X POST http://localhost:5000/api/nutrition/generate \
  -H "Content-Type: application/json" \
  -d '{"goal":"perte de poids","level":"d√©butant","weight":70,"height":175,"activity":"mod√©r√©e","preferences":"v√©g√©tarien"}'

# Test training plan
curl -X POST http://localhost:5000/api/trainingPlan/generate \
  -H "Content-Type: application/json" \
  -d '{"goal":"prise de muscle","level":"interm√©diaire","lastPlan":"aucun"}'
```

## üö® D√©pannage

### Ollama non disponible

Si vous voyez le message :
```
‚ö†Ô∏è Service IA temporairement indisponible. Relance Ollama.
```

**Solutions :**
1. V√©rifiez que Ollama est d√©marr√© : `ollama serve`
2. V√©rifiez que le mod√®le est install√© : `ollama list`
3. Installez le mod√®le si n√©cessaire : `ollama pull llama3.2:1b`

### Port d√©j√† utilis√©

Si le port 5000 est occup√© :
```bash
# Trouver le processus
lsof -i :5000

# Arr√™ter le processus
kill -9 <PID>
```

## üìä Performance

- **Mod√®le utilis√©** : llama3.2:1b (l√©ger et rapide)
- **Temps de r√©ponse** : ~2-8 secondes selon la complexit√©
- **Ressources** : ~1-2GB RAM pour le mod√®le

## üîÑ Migration depuis OpenAI

L'int√©gration est r√©trocompatible. Pour revenir √† OpenAI :

1. Remplacez `AI_PROVIDER=ollama` par `AI_PROVIDER=openai`
2. Ajoutez `OPENAI_API_KEY=your-key`
3. R√©installez OpenAI : `npm install openai`

## üìù Notes

- Le mod√®le `llama3.2:1b` est optimis√© pour les r√©ponses rapides
- Les r√©ponses JSON peuvent n√©cessiter un parsing c√¥t√© client
- Le fallback vers des plans par d√©faut est impl√©ment√© en cas d'erreur
- Tous les logs sont affich√©s dans la console du serveur
